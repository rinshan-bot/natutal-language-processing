{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIo3i8pQs6QZES0mfW/9/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rinshan-bot/natutal-language-processing/blob/main/Word_Representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "6cjPVd9n6yVC",
        "outputId": "ce7934c9-af84-4251-d45a-24a9ad4f821e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76eb93e0678f>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Get Brown cluster features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprefix_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath_prefix_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mpath_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_path_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath_prefix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mcluster_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath_prefix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_path_prefix' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense\n",
        "\n",
        "# Suppose we have the following data for training NER\n",
        "sentences = [\"John lives in New York City\", \"Mary works at a company in London\"]\n",
        "labels = [\"PERSON\", \"LOCATION\", \"LOCATION\"]\n",
        "\n",
        "# Suppose we have pre-trained word embeddings and Brown clusters\n",
        "word_embeddings = {}  # Pre-trained word embeddings\n",
        "brown_clusters = {}  # Pre-trained Brown clusters\n",
        "\n",
        "# Define the maximum path prefix sizes for Brown clusters\n",
        "path_prefix_sizes = [4, 6, 10, 20]\n",
        "\n",
        "# Prepare input features\n",
        "features = []\n",
        "for sentence in sentences:\n",
        "    sentence_features = []\n",
        "    for word in sentence.split():\n",
        "        # Get word embedding feature\n",
        "        if word in word_embeddings:\n",
        "            word_embedding_feature = word_embeddings[word]\n",
        "        else:\n",
        "            word_embedding_feature = np.zeros(300)  # Use zero vector for out-of-vocabulary words\n",
        "        sentence_features.append(word_embedding_feature)\n",
        "\n",
        "        # Get Brown cluster features\n",
        "        for prefix_size in path_prefix_sizes:\n",
        "            path_prefix = get_path_prefix(word, prefix_size)\n",
        "            if path_prefix in brown_clusters:\n",
        "                cluster_feature = brown_clusters[path_prefix]\n",
        "            else:\n",
        "                cluster_feature = np.zeros(12)  # Use zero vector for out-of-cluster words\n",
        "            sentence_features.append(cluster_feature)\n",
        "\n",
        "    features.append(sentence_features)\n",
        "\n",
        "# Convert features to numpy arrays\n",
        "features = np.array(features)\n",
        "\n",
        "# One-hot encode labels\n",
        "label_encoder = OneHotEncoder(sparse=False)\n",
        "labels_encoded = label_encoder.fit_transform(np.array(labels).reshape(-1, 1))\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=features.shape[1], output_dim=128, input_length=features.shape[1]))\n",
        "model.add(Dense(units=64, activation='relu'))\n",
        "model.add(Dense(units=label_encoder.categories_[0].shape[0], activation='softmax'))\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(features, labels_encoded, epochs=10, batch_size=32)\n",
        "\n",
        "# Predict on new data\n",
        "new_sentences = [\"Peter is visiting Paris\"]\n",
        "new_features = []\n",
        "for sentence in new_sentences:\n",
        "    new_sentence_features = []\n",
        "    for word in sentence.split():\n",
        "        # Get word embedding feature\n",
        "        if word in word_embeddings:\n",
        "            word_embedding_feature = word_embeddings[word]\n",
        "        else:\n",
        "            word_embedding_feature = np.zeros(300)\n",
        "        new_sentence_features.append(word_embedding_feature)\n",
        "\n",
        "        # Get Brown cluster features\n",
        "        for prefix_size in path_prefix_sizes:\n",
        "            path_prefix = get_path_prefix(word, prefix_size)\n",
        "            if path_prefix in brown_clusters:\n",
        "                cluster_feature = brown_clusters[path_prefix]\n",
        "            else:\n",
        "                cluster_feature = np.zeros(12)\n",
        "            new_sentence_features.append(cluster_feature)\n",
        "\n",
        "    new_features.append(new_sentence_features)\n",
        "\n",
        "new_features = np.array(new_features)\n",
        "predictions = model.predict(new_features)\n",
        "\n",
        "# Decode predicted labels\n",
        "predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))\n",
        "print(predicted_labels)\n"
      ]
    }
  ]
}